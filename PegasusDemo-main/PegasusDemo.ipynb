{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3277: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "c:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the story of how i ended up with a nasty gash on my leg.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"this happened 5/6 years ago so my whole family every xmas day goes around to my aunties for celebrations. my cousin (of course) was there and he\n",
    "asked if i wanted to play cops and robbers. i accepted of course. now, next to the side of my aunts house is a little area with a small fence, a covered\n",
    "water tank and super duper sharp stones. my cousin (who was the cop) was gaining on me. i (tried) to jump over the fence, aaand i failed the jump\n",
    "and went crashing onto the gravel, my leg hitting the sharpest bit and, then the next thing i knew it had a nasty gash.\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "# batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3277: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iâ€™ve been meaning to write this post for a long time.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"throwaway here for obvious reasons.. today my friends and i decided to go off-roading in nowhereland. we packed up all our stuff, made the roughly\n",
    "hour drive off to the mountains to make a fire, go fishing and just talk about life until we got too tired to stay any longer. we got everything packed\n",
    "up and brought along one of my friendsâ€™ dog because sheâ€™s awesome and loves the outdoors. the dog was flipping out in the suv on the way to the\n",
    "path because she knew was a kick-ass day she was about to have breaking out of her normally lame, domesticated dog life. my friends decided to\n",
    "drink during the off-roading adventure, which was fine because i volunteered to drive since i cannot drink alcohol (mouth is wired shut [long story\n",
    "but i canâ€™t drink alcohol for a while]) so we were playing it safe. the dog couldnâ€™t be any happier and was about to jump out of the truck (literally)\n",
    "when we got there so the dogâ€™s owner let her get out and run along side of us while we drove the dirt road up to the destination for the fire. as i was\n",
    "driving, the dog went in and out of vision, mostly biting the tires as most dogs do, playing around. the owner kept asking us (the two guys up front)\n",
    "if we could see her. we said yes, and kept driving. as i was driving at no more than 5-10mph along the dirt road, i could hear the dog biting at the\n",
    "tires playfully, but we just laughed it off bc we thought she was having fun. the horrible, seconds-long event that ensued was me feeling the dreaded\n",
    "â€™double-thudâ€™ under the tires and heard the dog yelp in pain. i instantly stopped the ... .\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "# batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Yash and I am a student of class 12.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\"\"\"My name is Yash. I'm 21 years old. I am the youngest of my only brother and two sisters. My elder brother is just six years elder in age than me. My other two elder sisters are younger than my brother. Being the eldest one, sometimes, gives you the favors. Luckily, I am loved, cared and most favored upon in my all family. I am the students of high school. My elder brother studies in university. One elder sister of mine is the college student whereas the the youngest sister of mine is studying class 10. Thatâ€™s, I go to school with my younger sister. I wake up early in the morning and finish up all of my doings on exact time. We all pack up for our respective works. My siblings prepare for school, college and university. My father, for hospital and my mother for nearby school. The values like hard work, time management, sincerity with work and dedication to purpose have been ingrained in our nature. Its true to say that a child becomes what the environment he gets in his home. All the essential humane qualities like sincerity, dedication, truth and honesty I do experience practically at my home. The school is a place that truly helps shaping oneâ€™s personality. My school is like another Godsend gift for me. It is one of the best school in our entire vicinity. We are truly cared, rightly educated and well socialized here at our school. I myself feel proud of being a student here. I do take care of what is being taught and what is to learn. I respect my teachers, my fellow beings, my school and every thing that is associated with my school. I am a good student, who actively participates in all activities, be it exams or extra curricular activities. The extracurricular activities like sports, quiz competitions, essays and speech competitions etc. are the heart of my institution. I myself am essay competition winner on various times. Truly it is said, one can not study from the books alone. Instead, one has to learn from all extracurricular activities in which one participates. I myself am the best essay writer, cricket player, singer and dancer and topper in all exams. The Career planning is the very thing that irritates oneself. Without a sound career planning, right from the start, one can not stay on the right track. One has to do the things or set the goals in accordance to his or her broad career goals. A man without a sound career planing is like a ship without the radar. It drifts and ultimately sinks down in the deep ocean. But unlike my father, who is a doctor, I myself feel a little interest in this profession. Though it is the best profession but oneâ€™s interest and aptitude differs. I myself wants to be a Civil servant to serve my people in my best capacity. I am very much inspired from my maternal uncle who is the chief of police force. That truly inspires me so as I myself want to serve my nation by being a good civil servant. Oneâ€™s future can be determined by what one has achieved today or what one is doing today. A man who dreams only for future and does nothing for tomorrow can be best named as  lazy and the pathetic person. In that way, I dislike myself being called as a man without a vision and castle maker in sky. I do work for what I dream. So far, I have proved myself being the best one in all fields like in academic, extracurricular and in personal domains. All of this success, hard work, dedication, determination is truly credited to my parents who have made this possible for me. Successes and failures are closely associated with oneâ€™s achievements and losses. One who achieves anything today has lost a thing yesterday. There is no completely successful man without the mixture of both failures and successes. That is to say, I failed a dozen of times. But, every failure made me stronger ever.\"\"\"]\n",
    "\n",
    "\n",
    "# batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-large'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the dog couldnâ€™t be any happier and was about to jump out of the truck (literally) when we got there so the dogâ€™s owner let her get out and run along side of us while we drove the dirt road up to the destination for the fire. as i was driving at no more than 5-10mph along the dirt road, i could hear the dog biting at the tires playfully, but we just laughed it off bc we thought she was having fun.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"throwaway here for obvious reasons.. today my friends and i decided to go off-roading in nowhereland. we packed up all our stuff, made the roughly\n",
    "hour drive off to the mountains to make a fire, go fishing and just talk about life until we got too tired to stay any longer. we got everything packed\n",
    "up and brought along one of my friendsâ€™ dog because sheâ€™s awesome and loves the outdoors. the dog was flipping out in the suv on the way to the\n",
    "path because she knew was a kick-ass day she was about to have breaking out of her normally lame, domesticated dog life. my friends decided to\n",
    "drink during the off-roading adventure, which was fine because i volunteered to drive since i cannot drink alcohol (mouth is wired shut [long story\n",
    "but i canâ€™t drink alcohol for a while]) so we were playing it safe. the dog couldnâ€™t be any happier and was about to jump out of the truck (literally)\n",
    "when we got there so the dogâ€™s owner let her get out and run along side of us while we drove the dirt road up to the destination for the fire. as i was\n",
    "driving, the dog went in and out of vision, mostly biting the tires as most dogs do, playing around. the owner kept asking us (the two guys up front)\n",
    "if we could see her. we said yes, and kept driving. as i was driving at no more than 5-10mph along the dirt road, i could hear the dog biting at the\n",
    "tires playfully, but we just laughed it off bc we thought she was having fun. the horrible, seconds-long event that ensued was me feeling the dreaded\n",
    "â€™double-thudâ€™ under the tires and heard the dog yelp in pain. i instantly stopped the ... .\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['But one day, when he turned eighty years old, an incredible thing happened. Instantly everyone started hearing the rumour: An Old Man is happy today, he doesnâ€™t complain about anything, smiles, and even his face is freshened up.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"An old man lived in the village. He was one of the most unfortunate people in the world. The whole village was tired of him; he was always gloomy, he constantly complained and was always in a bad mood. The longer he lived, the more bile he was becoming and the more poisonous were his words. People avoided him, because his misfortune became contagious. It was even unnatural and insulting to be happy next to him.He created the feeling of unhappiness in others. But one day, when he turned eighty years old, an incredible thing happened. Instantly everyone started hearing the rumour: An Old Man is happy today, he doesnâ€™t complain about anything, smiles, and even his face is freshened up. The whole village gathered together. The old man was asked: Villager: What happened to you? Nothing special. Eighty years I've been chasing happiness, and it was useless. And then I decided to live without happiness and just enjoy life. That's why I'm happy now.\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now, next to the side of my aunts house is a little area with a small fence, a covered water tank and super duper sharp stones.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"this happened 5/6 years ago so my whole family every xmas day goes around to my aunties for celebrations. my cousin (of course) was there and he\n",
    "asked if i wanted to play cops and robbers. i accepted of course. now, next to the side of my aunts house is a little area with a small fence, a covered\n",
    "water tank and super duper sharp stones. my cousin (who was the cop) was gaining on me. i (tried) to jump over the fence, aaand i failed the jump\n",
    "and went crashing onto the gravel, my leg hitting the sharpest bit and, then the next thing i knew it had a nasty gash.\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-reddit_tifu'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"played cops and robbers, tried to jump over a fence, ended up with a nasty gash on my leg. i'm not a good cop.\"]\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"this happened 5/6 years ago so my whole family every xmas day goes around to my aunties for celebrations. my cousin (of course) was there and he\n",
    "asked if i wanted to play cops and robbers. i accepted of course. now, next to the side of my aunts house is a little area with a small fence, a covered\n",
    "water tank and super duper sharp stones. my cousin (who was the cop) was gaining on me. i (tried) to jump over the fence, aaand i failed the jump\n",
    "and went crashing onto the gravel, my leg hitting the sharpest bit and, then the next thing i knew it had a nasty gash.\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['went off-roading with friends and brought a friendâ€™s dog along. the dog went in and out of vision, biting the tires, and i felt the dreaded â€™double-thudâ€™ under the tires and heard the dog yelp in pain.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\n",
    "    \"\"\"throwaway here for obvious reasons.. today my friends and i decided to go off-roading in nowhereland. we packed up all our stuff, made the roughly\n",
    "hour drive off to the mountains to make a fire, go fishing and just talk about life until we got too tired to stay any longer. we got everything packed\n",
    "up and brought along one of my friendsâ€™ dog because sheâ€™s awesome and loves the outdoors. the dog was flipping out in the suv on the way to the\n",
    "path because she knew was a kick-ass day she was about to have breaking out of her normally lame, domesticated dog life. my friends decided to\n",
    "drink during the off-roading adventure, which was fine because i volunteered to drive since i cannot drink alcohol (mouth is wired shut [long story\n",
    "but i canâ€™t drink alcohol for a while]) so we were playing it safe. the dog couldnâ€™t be any happier and was about to jump out of the truck (literally)\n",
    "when we got there so the dogâ€™s owner let her get out and run along side of us while we drove the dirt road up to the destination for the fire. as i was\n",
    "driving, the dog went in and out of vision, mostly biting the tires as most dogs do, playing around. the owner kept asking us (the two guys up front)\n",
    "if we could see her. we said yes, and kept driving. as i was driving at no more than 5-10mph along the dirt road, i could hear the dog biting at the\n",
    "tires playfully, but we just laughed it off bc we thought she was having fun. the horrible, seconds-long event that ensued was me feeling the dreaded\n",
    "â€™double-thudâ€™ under the tires and heard the dog yelp in pain. i instantly stopped the ... .\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest', return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
