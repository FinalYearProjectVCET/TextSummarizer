{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4908ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,LSTM,Bidirectional,Flatten,Dropout,BatchNormalization,Embedding,Input,TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14cba8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data=[]\n",
    "Articles_with_stopwords=[]\n",
    "Articles_without_stopwords=[]\n",
    "Summaries=[]\n",
    "stop_words=set(stopwords.words('english'))\n",
    "for d,path,filenames in tqdm(os.walk('BBC News Summary')):\n",
    "#     print(\"d \",d)\n",
    "#     print(\"path \",path)\n",
    "#     print(\"filenames \",filenames)\n",
    "    for file in filenames:\n",
    "        if os.path.isfile(d+'/'+file):            \n",
    "#             print(d+'/'+file)\n",
    "            if('Summaries' in d+'/'+file):\n",
    "                with open(d+'/'+file,'r',errors='ignore') as f:\n",
    "                    summary=''.join([i.rstrip() for i in f.readlines()])                    \n",
    "                    Summaries.append(summary)                    \n",
    "                    f.close()                       \n",
    "            else:\n",
    "                with open(d+'/'+file,'r',errors='ignore') as f:\n",
    "                    Article=''.join([i.rstrip() for i in f.readlines()])\n",
    "                    Articles_with_stopwords.append(Article)\n",
    "                    Articles_without_stopwords.append(' '.join([w for w in Article.split() if w not in stop_words]))\n",
    "                    f.close()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7781c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(Articles_with_stopwords),len(Articles_without_stopwords),len(Summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea91a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles without stop words</th>\n",
       "      <th>Article with stop words</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Articles without stop words, Article with stop words, Summary]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame({'Articles without stop words':Articles_without_stopwords,'Article with stop words': Articles_with_stopwords,'Summary':Summaries})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5c0469",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5240/265788684.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Articles without stop words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "data['Articles without stop words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(data['Article with stop words'][i])\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35713c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text=text.lower()    \n",
    "    text=' '.join([contraction_mapping[i] if i in contraction_mapping.keys() else i for i in text.split()])\n",
    "    text=re.sub(r'\\(.*\\)',\"\",text)\n",
    "    text=re.sub(\"'s\",\"\",text)\n",
    "    text=re.sub('\"','',text)\n",
    "    text=' '.join([i for i in text.split() if i.isalpha()])\n",
    "    text=re.sub('[^a-zA-Z]',\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c59785",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"Yash's (VCET) \\\"hello\\\" isn't bad abc850\"\n",
    "\n",
    "clean_text(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Article with stop words']=data['Article with stop words'].apply(clean_text)\n",
    "data['Articles without stop words']=data['Articles without stop words'].apply(clean_text)\n",
    "data['Summary']=data['Summary'].apply(clean_text)\n",
    "data['Summary']='<START> '+data['Summary']+' <END>'\n",
    "# print(data['Summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Article with stop words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Articles without stop words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(data['Article with stop words'],data['Summary'],test_size=0.3,random_state=29)\n",
    "print(len(X_train),len(Y_train))\n",
    "print(len(X_val),len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aee403",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_len=[len(i.split()) for i in X_train]\n",
    "sum_len=[len(i.split()) for i in Y_train]\n",
    "# print(art_len[0], sum_len[0])\n",
    "plt.hist(art_len,bins=100)\n",
    "plt.title('Article')\n",
    "plt.xlabel(\"Length of Articles\")\n",
    "plt.ylabel(\"No. of Articles of that length\")\n",
    "plt.show()\n",
    "plt.hist(sum_len,bins=50)\n",
    "plt.title('Summary')\n",
    "plt.xlabel(\"Length of Summary\")\n",
    "plt.ylabel(\"No. of Articles of that length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_art_len=500\n",
    "max_sum_len=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tokenizer=Tokenizer(oov_token='<UNK>')\n",
    "article_tokenizer.fit_on_texts(X_train)\n",
    "tokenized_X_train=article_tokenizer.texts_to_sequences(X_train)\n",
    "tokenized_X_val=article_tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_vocab_size=len(article_tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train=pad_sequences(tokenized_X_train,maxlen=max_art_len,padding='post',truncating='post')\n",
    "padded_X_val=pad_sequences(tokenized_X_val,maxlen=max_art_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35128aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_X_train.shape,padded_X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afab5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tokenizer=Tokenizer(oov_token='<UNK>')\n",
    "summary_tokenizer.fit_on_texts(Y_train)\n",
    "tokenized_Y_train=summary_tokenizer.texts_to_sequences(Y_train)\n",
    "tokenized_Y_val=summary_tokenizer.texts_to_sequences(Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_vocab_size=len(summary_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_Y_train=pad_sequences(tokenized_Y_train,maxlen=max_sum_len,padding='post',truncating='post')\n",
    "padded_Y_val=pad_sequences(tokenized_Y_val,maxlen=max_sum_len,padding='post',truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_Y_train.shape,padded_Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_art_index=article_tokenizer.index_word\n",
    "reverse_sum_index=summary_tokenizer.index_word\n",
    "sum_wordindex=summary_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c39597",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs=Input(shape=(max_art_len,))\n",
    "encoder_emb=Embedding(art_vocab_size,100,trainable=True,name='Encoder_Embedding_layer')(encoder_inputs)\n",
    "encoder_lstm1=LSTM(300,return_sequences=True,return_state=True,name='Encoder_LSTM1')\n",
    "enclstm1_outputs,enclstm1_h,enclstm1_c=encoder_lstm1(encoder_emb)\n",
    "\n",
    "\n",
    "decoder_inputs=Input(shape=(None,))\n",
    "decoder_em=Embedding(sum_vocab_size,100,trainable=True,name='Decoder_Embedding_layer')\n",
    "decoder_emb=decoder_em(decoder_inputs)\n",
    "\n",
    "decoder_lstm1=LSTM(300,return_sequences=True,return_state=True,name='Decoder_LSTM1')\n",
    "declstm1_output,declstm1_h,declstm1_c=decoder_lstm1(decoder_emb,initial_state=[enclstm1_h,enclstm1_c])\n",
    "\n",
    "output_layer=TimeDistributed(Dense(sum_vocab_size,activation='softmax',name='softmax'))\n",
    "output=output_layer(declstm1_output)\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([padded_X_train,padded_Y_train[:,:-1]],padded_Y_train[:,1:],\n",
    "          epochs=1,\n",
    "          validation_data=([padded_X_val,padded_Y_val[:,:-1]],padded_Y_val[:,1:]),\n",
    "          batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25457a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "infencoder_model=Model(inputs=[encoder_inputs],outputs=[enclstm1_outputs,enclstm1_h,enclstm1_c])\n",
    "\n",
    "infdecoder_model_state_input_h=Input(shape=(300,),name='infdec_I1')\n",
    "infdecoder_model_state_input_c=Input(shape=(300,),name='infdec_I2')\n",
    "\n",
    "infdeclstm1_output,infdec_h,infdec_c=decoder_lstm1(decoder_emb,initial_state=[infdecoder_model_state_input_h,\n",
    "                                                                                                infdecoder_model_state_input_c\n",
    "                                                                                               ])\n",
    "\n",
    "infdec_output=output_layer(infdeclstm1_output)                         \n",
    "\n",
    "infdecoder_model=Model(inputs=[decoder_inputs]+[infdecoder_model_state_input_h,infdecoder_model_state_input_c],\n",
    "                       outputs=[infdec_output]+[infdec_h,infdec_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infencoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(infencoder_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ca116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd925540",
   "metadata": {},
   "outputs": [],
   "source": [
    "infdecoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(infdecoder_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(inp_seq):\n",
    "    \n",
    "    enc_out,enc_h,enc_c=infencoder_model.predict(inp_seq)\n",
    "    \n",
    "    tar_seq=np.zeros((1,1))\n",
    "    tar_seq[0,0]=sum_wordindex['start']\n",
    "    \n",
    "    stop_loop=False\n",
    "    decoded_string=''\n",
    "    \n",
    "    while not stop_loop:\n",
    "       \n",
    "        dec_out,dec_h,dec_c=infdecoder_model.predict([tar_seq]+[enc_h,enc_c])\n",
    "        \n",
    "        tar_token_index=np.argmax(dec_out[0,-1,:])\n",
    "        tar_token_word=sum_wordindex[tar_token_index]\n",
    "        \n",
    "        if tar_token_word =='end' or len(decoded_string)>=max_art_len:\n",
    "            \n",
    "            stop_loop=True\n",
    "        else:\n",
    "            decoded_string+=tar_token_word\n",
    "            \n",
    "            tar_seq=np.zeros((1,1))\n",
    "            tar_seq[0,0]=tar_token_index\n",
    "            \n",
    "            \n",
    "            enc_h=dec_h\n",
    "            enc_c=dec_c\n",
    "            \n",
    "    return decoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0eeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2art(inp_seq):\n",
    "    \n",
    "    art=''\n",
    "    \n",
    "    for i in range(len(inp_seq)):\n",
    "        \n",
    "        if inp_seq[i]==0:\n",
    "            break\n",
    "        art+=reverse_art_index[inp_seq[i]]+' '\n",
    "        \n",
    "    return art\n",
    "\n",
    "\n",
    "def seq2sum(inp_seq):\n",
    "    \n",
    "    summary=''\n",
    "    \n",
    "    for i in range(len(inp_seq)):\n",
    "        \n",
    "        if inp_seq[i]==0:\n",
    "            break\n",
    "        word=reverse_sum_index[inp_seq[i]]\n",
    "        summary+=word+' '\n",
    "            \n",
    "    return summary\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example Articel : '+'\\n',seq2art(padded_X_val[2]))\n",
    "print('Example Summary : '+'\\n',seq2sum(padded_Y_val[2]))\n",
    "print('Predicted Summary : '+'\\n',decode_sequence(padded_X_val[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257b216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
